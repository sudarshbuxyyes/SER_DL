{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nfrom torchvision import models\nimport torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:53:48.268322Z","iopub.execute_input":"2022-04-29T16:53:48.268581Z","iopub.status.idle":"2022-04-29T16:53:52.092813Z","shell.execute_reply.started":"2022-04-29T16:53:48.268552Z","shell.execute_reply":"2022-04-29T16:53:52.091971Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!cd ../input/berlin-database-of-emotional-speech-emodb","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:53:52.094750Z","iopub.execute_input":"2022-04-29T16:53:52.095166Z","iopub.status.idle":"2022-04-29T16:53:52.837120Z","shell.execute_reply.started":"2022-04-29T16:53:52.095130Z","shell.execute_reply":"2022-04-29T16:53:52.836191Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\ndata_dir = \"../input/berlin-database-of-emotional-speech-emodb/wav\"\nfile_path_list = os.listdir(data_dir)\nprint(len(np.unique(file_path_list)))\nfile_path_list = [data_dir + \"/\" + file_path for file_path in file_path_list]\nEMOTIONS = {1:'angry', 2:'bored', 3:'disgust', 4:'fear', 5:'happy', 6:'sad', 0:'neutral'} \n# DATA_PATH = '../input/ravdess-emotional-speech-audio/'\nSAMPLE_RATE = 48000\n\n# data = pd.DataFrame(columns=['letter', 'motion', 'Gender','Path'])\n# for filename in file_path_list:\nprint(len(file_path_list))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:53:52.841614Z","iopub.execute_input":"2022-04-29T16:53:52.841922Z","iopub.status.idle":"2022-04-29T16:53:52.901519Z","shell.execute_reply.started":"2022-04-29T16:53:52.841885Z","shell.execute_reply":"2022-04-29T16:53:52.900820Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"emotion = []\npath = []\nfor root, dirs, files in os.walk(\"../input/berlin-database-of-emotional-speech-emodb/wav\"):\n    print(len(root), len(dirs), len(files))\n    for name in files:\n#         print(name)\n#         if name[0:2] in '0310111215':  # MALE\n#             if name[5] == 'W':  # Ärger (Wut) -> Angry\n#                 emotion.append('angry')\n#             elif name[5] == 'L':  # Langeweile -> Boredom\n#                 emotion.append('bored')\n#             elif name[5] == 'E':  # Ekel -> Disgusted\n#                 emotion.append('disgust')\n#             elif name[5] == 'A':  # Angst -> Angry\n#                 emotion.append('fear')\n#             elif name[5] == 'F':  # Freude -> Happiness\n#                 emotion.append('happy')\n#             elif name[5] == 'T':  # Trauer -> Sadness\n#                 emotion.append('sad')\n#             elif name[6] == 'N':\n#                 emotion.append('neutral')\n#             else:\n#                 emotion.append('unknown')\n#         else:\n        if name[5] == 'W':  # Ärger (Wut) -> Angry\n            emotion.append('angry')\n        elif name[5] == 'L':  # Langeweile -> Boredom\n            emotion.append('bored')\n        elif name[5] == 'E':  # Ekel -> Disgusted\n            emotion.append('disgust')\n        elif name[5] == 'A':  # Angst -> Angry\n            emotion.append('fear')\n        elif name[5] == 'F':  # Freude -> Happiness\n            emotion.append('happy')\n        elif name[5] == 'T':  # Trauer -> Sadness\n            emotion.append('sad')\n        elif name[5] == 'N':\n            emotion.append('neutral')\n        else:\n            emotion.append('unknown')\n\n        path.append(os.path.join(\"../input/berlin-database-of-emotional-speech-emodb/wav\", name))\n\nemodb_df = pd.DataFrame(emotion, columns=['Emotion'])\nemodb_df['source'] = 'EMODB'\n# emodb_df = pd.concat([emodb_df, pd.DataFrame(path, columns=['Path'])], axis=1)\nemodb_df[\"Path\"] = path","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:53:52.906654Z","iopub.execute_input":"2022-04-29T16:53:52.908880Z","iopub.status.idle":"2022-04-29T16:53:52.944226Z","shell.execute_reply.started":"2022-04-29T16:53:52.908840Z","shell.execute_reply":"2022-04-29T16:53:52.943524Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"emodb_df\ndata = emodb_df.dropna()\ndata","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:53:52.948124Z","iopub.execute_input":"2022-04-29T16:53:52.950183Z","iopub.status.idle":"2022-04-29T16:53:52.985399Z","shell.execute_reply.started":"2022-04-29T16:53:52.950144Z","shell.execute_reply":"2022-04-29T16:53:52.984702Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot(111)\nax.bar(x=range(7), height=emodb_df['Emotion'].value_counts())\nax.set_xticks(ticks=range(7))\nax.set_xticklabels([EMOTIONS[i] for i in range(7)],fontsize=10)\nax.set_xlabel('Emotions')\nax.set_ylabel('Number of examples')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:53:52.988199Z","iopub.execute_input":"2022-04-29T16:53:52.988399Z","iopub.status.idle":"2022-04-29T16:53:53.211232Z","shell.execute_reply.started":"2022-04-29T16:53:52.988374Z","shell.execute_reply":"2022-04-29T16:53:53.210498Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data = emodb_df\n#Dropping rows in df with NaN values\ndata = data.dropna()\nprint(len(data))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:53:53.212600Z","iopub.execute_input":"2022-04-29T16:53:53.212886Z","iopub.status.idle":"2022-04-29T16:53:53.219312Z","shell.execute_reply.started":"2022-04-29T16:53:53.212852Z","shell.execute_reply":"2022-04-29T16:53:53.218624Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(data.head())","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:53:53.220643Z","iopub.execute_input":"2022-04-29T16:53:53.221119Z","iopub.status.idle":"2022-04-29T16:53:53.230561Z","shell.execute_reply.started":"2022-04-29T16:53:53.221084Z","shell.execute_reply":"2022-04-29T16:53:53.229700Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"data['Emotion'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:53:53.231999Z","iopub.execute_input":"2022-04-29T16:53:53.232254Z","iopub.status.idle":"2022-04-29T16:53:53.240646Z","shell.execute_reply.started":"2022-04-29T16:53:53.232219Z","shell.execute_reply":"2022-04-29T16:53:53.239881Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\nmel_spectrograms = []\nsignals = []\nfor i, file_path in enumerate(data.Path):\n    audio, sample_rate = librosa.load(file_path, duration=3, offset=0.5, sr=SAMPLE_RATE)\n    signal = np.zeros((int(SAMPLE_RATE*3,)))\n    signal[:len(audio)] = audio\n    signals.append(signal)\n    print(\"\\r Processed {}/{} files\".format(i,len(data)),end='')\nsignals = np.stack(signals,axis=0)\nprint(type(signals))\nprint(signals)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:53:53.244149Z","iopub.execute_input":"2022-04-29T16:53:53.244352Z","iopub.status.idle":"2022-04-29T16:54:44.725275Z","shell.execute_reply.started":"2022-04-29T16:53:53.244323Z","shell.execute_reply":"2022-04-29T16:54:44.724402Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"X = signals\nX = np.array(X)\ntrain_ind,test_ind,val_ind = [],[],[]\nX_train,X_val,X_test = [],[],[]\nY_train,Y_val,Y_test = [],[],[]\nfor emotion in range(len(EMOTIONS)):\n    em = EMOTIONS[emotion]\n#     print(em)\n    emotion_ind = list(data.loc[data.Emotion==em,'Emotion'].index)\n#     print(np.sum(data.Emotion==em))\n#     print(emotion_ind)\n    emotion_ind = np.random.permutation(emotion_ind)\n    m = len(emotion_ind)\n    ind_train = emotion_ind[:int(0.8*m)]\n#     print(ind_train)\n    ind_val = emotion_ind[int(0.8*m):int(0.9*m)]\n    ind_test = emotion_ind[int(0.9*m):]\n    X_train.append(X[ind_train,:])\n    Y_train.append(np.array([emotion]*len(ind_train),dtype=np.int32))\n    X_val.append(X[ind_val,:])\n    Y_val.append(np.array([emotion]*len(ind_val),dtype=np.int32))\n    X_test.append(X[ind_test,:])\n    Y_test.append(np.array([emotion]*len(ind_test),dtype=np.int32))\n    train_ind.append(ind_train)\n    test_ind.append(ind_test)\n    val_ind.append(ind_val)\nX_train = np.concatenate(X_train,0)\nX_val = np.concatenate(X_val,0)\nX_test = np.concatenate(X_test,0)\nY_train = np.concatenate(Y_train,0)\nY_val = np.concatenate(Y_val,0)\nY_test = np.concatenate(Y_test,0)\ntrain_ind = np.concatenate(train_ind,0)\nval_ind = np.concatenate(val_ind,0)\ntest_ind = np.concatenate(test_ind,0)\nprint(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\nprint(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\nprint(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')\n# check if all are unique\nunique, count = np.unique(np.concatenate([train_ind,test_ind,val_ind],0), return_counts=True)\nprint(\"Number of unique indexes is {}, out of {}\".format(sum(count==1), X.shape[0]))\n\ndel X","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:54:44.726605Z","iopub.execute_input":"2022-04-29T16:54:44.727844Z","iopub.status.idle":"2022-04-29T16:54:45.451777Z","shell.execute_reply.started":"2022-04-29T16:54:44.727802Z","shell.execute_reply":"2022-04-29T16:54:45.451005Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def addAWGN(signal, num_bits=16, augmented_num=2, snr_low=15, snr_high=30): \n    signal_len = len(signal)\n    # Generate White Gaussian noise\n    noise = np.random.normal(size=(augmented_num, signal_len))\n    # Normalize signal and noise\n    norm_constant = 2.0**(num_bits-1)\n    signal_norm = signal / norm_constant\n    noise_norm = noise / norm_constant\n    # Compute signal and noise power\n    s_power = np.sum(signal_norm ** 2) / signal_len\n    n_power = np.sum(noise_norm ** 2, axis=1) / signal_len\n    # Random SNR: Uniform [15, 30] in dB\n    target_snr = np.random.randint(snr_low, snr_high)\n    # Compute K (covariance matrix) for each noise \n    K = np.sqrt((s_power / n_power) * 10 ** (- target_snr / 10))\n    K = np.ones((signal_len, augmented_num)) * K  \n    # Generate noisy signal\n    return signal + K.T * noise","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:54:45.453110Z","iopub.execute_input":"2022-04-29T16:54:45.453523Z","iopub.status.idle":"2022-04-29T16:54:45.461047Z","shell.execute_reply.started":"2022-04-29T16:54:45.453486Z","shell.execute_reply":"2022-04-29T16:54:45.460390Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"aug_signals = []\naug_labels = []\nfor i in range(X_train.shape[0]):\n    signal = X_train[i,:]\n    augmented_signals = addAWGN(signal)\n    for j in range(augmented_signals.shape[0]):\n        em = data.loc[i, \"Emotion\"]\n        aug_labels.append(list(EMOTIONS.keys())[list(EMOTIONS.values()).index(em)])\n        aug_signals.append(augmented_signals[j,:])\n        data = data.append(data.iloc[i], ignore_index=True)\n    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\naug_signals = np.stack(aug_signals,axis=0)\nX_train = np.concatenate([X_train,aug_signals],axis=0)\naug_labels = np.stack(aug_labels,axis=0)\nY_train = np.concatenate([Y_train,aug_labels])\nprint('')\nprint(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:54:45.462301Z","iopub.execute_input":"2022-04-29T16:54:45.462616Z","iopub.status.idle":"2022-04-29T16:54:53.022207Z","shell.execute_reply.started":"2022-04-29T16:54:45.462580Z","shell.execute_reply":"2022-04-29T16:54:53.021377Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_train = X_train[:(32 * (X_train.shape[0]//32))]\nY_train = Y_train[:(32 * (Y_train.shape[0]//32))]\nX_val = X_val[:(32 * (X_val.shape[0]//32))]\nY_val = Y_val[:(32 * (Y_val.shape[0]//32))]\nX_test = X_test[:(32 * (X_test.shape[0]//32))]\nY_test = Y_test[:(32 * (Y_test.shape[0]//32))]","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:54:53.023392Z","iopub.execute_input":"2022-04-29T16:54:53.023733Z","iopub.status.idle":"2022-04-29T16:54:53.029779Z","shell.execute_reply.started":"2022-04-29T16:54:53.023695Z","shell.execute_reply":"2022-04-29T16:54:53.029084Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\ndef getMELspectrogram(audio, sample_rate):\n    mel_spec = librosa.feature.melspectrogram(y=audio,\n                                              sr=sample_rate,\n                                              n_fft=1024,\n                                              win_length = 512,\n                                              window='hamming',\n                                              hop_length = 256,\n                                              n_mels=128,\n                                              fmax=sample_rate/2\n                                             )\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n    return mel_spec_db\n\n# test function\naudio, sample_rate = librosa.load(data.loc[0,'Path'], duration=3, offset=0.5,sr=SAMPLE_RATE)\nsignal = np.zeros((int(SAMPLE_RATE*3,)))\nsignal[:len(audio)] = audio\nmel_spectrogram = getMELspectrogram(signal, SAMPLE_RATE)\nlibrosa.display.specshow(mel_spectrogram, y_axis='mel', x_axis='time')\nprint('MEL spectrogram shape: ',mel_spectrogram.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:54:53.030883Z","iopub.execute_input":"2022-04-29T16:54:53.031389Z","iopub.status.idle":"2022-04-29T16:54:53.445758Z","shell.execute_reply.started":"2022-04-29T16:54:53.031353Z","shell.execute_reply":"2022-04-29T16:54:53.444940Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"mel_train = []\nprint(\"Calculatin mel spectrograms for train set\")\nfor i in range(X_train.shape[0]):\n    mel_spectrogram = getMELspectrogram(X_train[i,:], sample_rate=SAMPLE_RATE)\n    mel_train.append(mel_spectrogram)\n    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\nprint('')\ndel X_train\n\nmel_val = []\nprint(\"Calculatin mel spectrograms for val set\")\nfor i in range(X_val.shape[0]):\n    mel_spectrogram = getMELspectrogram(X_val[i,:], sample_rate=SAMPLE_RATE)\n    mel_val.append(mel_spectrogram)\n    print(\"\\r Processed {}/{} files\".format(i,X_val.shape[0]),end='')\nprint('')\ndel X_val\n\nmel_test = []\nprint(\"Calculatin mel spectrograms for test set\")\nfor i in range(X_test.shape[0]):\n    mel_spectrogram = getMELspectrogram(X_test[i,:], sample_rate=SAMPLE_RATE)\n    mel_test.append(mel_spectrogram)\n    print(\"\\r Processed {}/{} files\".format(i,X_test.shape[0]),end='')\nprint('')\ndel X_test","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:54:53.447133Z","iopub.execute_input":"2022-04-29T16:54:53.447561Z","iopub.status.idle":"2022-04-29T16:55:19.012779Z","shell.execute_reply.started":"2022-04-29T16:54:53.447524Z","shell.execute_reply":"2022-04-29T16:55:19.011984Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def splitIntoChunks(mel_spec,win_size,stride):\n    t = mel_spec.shape[1]\n    num_of_chunks = int(t/stride)\n    chunks = []\n    for i in range(num_of_chunks):\n        chunk = mel_spec[:,i*stride:i*stride+win_size]\n        if chunk.shape[1] == win_size:\n            chunks.append(chunk)\n    return np.stack(chunks,axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:55:19.017462Z","iopub.execute_input":"2022-04-29T16:55:19.018082Z","iopub.status.idle":"2022-04-29T16:55:19.028932Z","shell.execute_reply.started":"2022-04-29T16:55:19.018039Z","shell.execute_reply":"2022-04-29T16:55:19.028101Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# get chunks\n# train set\nmel_train_chunked = []\nfor mel_spec in mel_train:\n    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n    mel_train_chunked.append(chunks)\nprint(\"Number of chunks is {}\".format(chunks.shape[0]))\n# val set\nmel_val_chunked = []\nfor mel_spec in mel_val:\n    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n    mel_val_chunked.append(chunks)\nprint(\"Number of chunks is {}\".format(chunks.shape[0]))\n# test set\nmel_test_chunked = []\nfor mel_spec in mel_test:\n    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n    mel_test_chunked.append(chunks)\nprint(\"Number of chunks is {}\".format(chunks.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:55:19.030542Z","iopub.execute_input":"2022-04-29T16:55:19.031045Z","iopub.status.idle":"2022-04-29T16:55:19.678727Z","shell.execute_reply.started":"2022-04-29T16:55:19.031008Z","shell.execute_reply":"2022-04-29T16:55:19.677933Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n# BATCH FIRST TimeDistributed layer\nclass TimeDistributed(nn.Module):\n    def __init__(self, module):\n        super(TimeDistributed, self).__init__()\n        self.module = module\n\n    def forward(self, x):\n\n        if len(x.size()) <= 2:\n            return self.module(x)\n        # squash samples and timesteps into a single axis\n        elif len(x.size()) == 3: # (samples, timesteps, inp1)\n            x_reshape = x.contiguous().view(-1, x.size(2))  # (samples * timesteps, inp1)\n        elif len(x.size()) == 4: # (samples,timesteps,inp1,inp2)\n            x_reshape = x.contiguous().view(-1, x.size(2), x.size(3)) # (samples*timesteps,inp1,inp2)\n        else: # (samples,timesteps,inp1,inp2,inp3)\n            x_reshape = x.contiguous().view(-1, x.size(2), x.size(3),x.size(4)) # (samples*timesteps,inp1,inp2,inp3)\n            \n        y = self.module(x_reshape)\n        \n        # we have to reshape Y\n        if len(x.size()) == 3:\n            y = y.contiguous().view(x.size(0), -1, y.size(1))  # (samples, timesteps, out1)\n        elif len(x.size()) == 4:\n            y = y.contiguous().view(x.size(0), -1, y.size(1), y.size(2)) # (samples, timesteps, out1,out2)\n        else:\n            y = y.contiguous().view(x.size(0), -1, y.size(1), y.size(2),y.size(3)) # (samples, timesteps, out1,out2, out3)\n        return y","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:55:19.680048Z","iopub.execute_input":"2022-04-29T16:55:19.680463Z","iopub.status.idle":"2022-04-29T16:55:19.691489Z","shell.execute_reply.started":"2022-04-29T16:55:19.680424Z","shell.execute_reply":"2022-04-29T16:55:19.690723Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:55:19.694443Z","iopub.execute_input":"2022-04-29T16:55:19.694658Z","iopub.status.idle":"2022-04-29T16:55:19.704138Z","shell.execute_reply.started":"2022-04-29T16:55:19.694632Z","shell.execute_reply":"2022-04-29T16:55:19.703408Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"pretrained_alexnet = models.alexnet(pretrained = True)\npretrained_alexnet.eval","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:55:19.705757Z","iopub.execute_input":"2022-04-29T16:55:19.706077Z","iopub.status.idle":"2022-04-29T16:55:25.423515Z","shell.execute_reply.started":"2022-04-29T16:55:19.706041Z","shell.execute_reply":"2022-04-29T16:55:25.422708Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"set_parameter_requires_grad(pretrained_alexnet, True)\nnum_ftrs2 = pretrained_alexnet.classifier[6].in_features\npretrained_alexnet.classifier[6] = nn.Linear(num_ftrs2, 1024)\npretrained_alexnet.eval","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:55:25.424877Z","iopub.execute_input":"2022-04-29T16:55:25.425195Z","iopub.status.idle":"2022-04-29T16:55:25.469201Z","shell.execute_reply.started":"2022-04-29T16:55:25.425159Z","shell.execute_reply":"2022-04-29T16:55:25.468500Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"pretrained = models.resnet18(pretrained = True)\nset_parameter_requires_grad(pretrained, True)\nnum_ftrs = pretrained.fc.in_features\npretrained.fc = nn.Linear(num_ftrs, 1024)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:55:25.470279Z","iopub.execute_input":"2022-04-29T16:55:25.470519Z","iopub.status.idle":"2022-04-29T16:55:27.205138Z","shell.execute_reply.started":"2022-04-29T16:55:25.470487Z","shell.execute_reply":"2022-04-29T16:55:27.204426Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class HybridModel(nn.Module):\n    def __init__(self,num_emotions):\n        super().__init__()\n        # conv block\n        #self.alexnet = pretrained\n        self.alexnet = pretrained_alexnet\n        self.conv_2d_1 = nn.Sequential(\n            nn.Conv2d(in_channels = 1,\n                      out_channels = 3,\n                      kernel_size = 3,\n                      stride = 1,\n                      padding = 1)\n        )\n        self.conv2Dblock1 = nn.Sequential(\n            # 1. conv block\n            TimeDistributed(nn.Conv2d(in_channels=1,\n                                   out_channels=16,\n                                   kernel_size=3,\n                                   stride=1,\n                                   padding=1\n                                  )),\n            TimeDistributed(nn.BatchNorm2d(16)),\n            TimeDistributed(nn.ReLU()),\n            TimeDistributed(nn.MaxPool2d(kernel_size=2, stride=2)),\n            TimeDistributed(nn.Dropout(p=0.3)))\n        self.conv2Dblock2 = nn.Sequential(\n            TimeDistributed(nn.Conv2d(in_channels=16,\n                                   out_channels=32,\n                                   kernel_size=3,\n                                   stride=1,\n                                   padding=1\n                                  )),\n            TimeDistributed(nn.BatchNorm2d(32)),\n            TimeDistributed(nn.ReLU()),\n            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n            TimeDistributed(nn.Dropout(p=0.3)),\n        )\n        self.conv2Dblock3 = nn.Sequential(\n            TimeDistributed(nn.Conv2d(in_channels=32,\n                                   out_channels=64,\n                                   kernel_size=3,\n                                   stride=1,\n                                   padding=1\n                                  )),\n            TimeDistributed(nn.BatchNorm2d(64)),\n            TimeDistributed(nn.ReLU()),\n            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n            TimeDistributed(nn.Dropout(p=0.3))\n        )\n        # LSTM block\n        hidden_size = 64\n        self.lstm = nn.LSTM(input_size=1024,hidden_size=hidden_size,bidirectional=True, batch_first=True)\n        self.dropout_lstm = nn.Dropout(p=0.4)\n        self.attention_linear = nn.Linear(2*hidden_size,1) # 2*hidden_size for the 2 outputs of bidir LSTM\n        # Linear softmax layer\n        self.out_linear = nn.Linear(2*hidden_size,num_emotions)\n\n    def forward(self,x):\n\n        # print(\"shape x:\", x.shape)\n        td_op = TimeDistributed(self.conv_2d_1)(x)\n        # print(\"Time distributed op: \", td_op.shape)\n\n        td_op = torch.reshape(td_op, [224, 3, 128, 128])\n        # print(\"Time distributed op: \", td_op.shape)\n        # conv_op = self.conv_2d_1(x)\n        # print(\"Convolution output: \", td_op)\n        # print(td_op.shape)\n        # print(type(td_op))\n        alexnet_out = self.alexnet(td_op)\n        # print(\"shape after td resnet:\", alexnet_out.shape)\n        #alexnet_out = alexnet_out.reshape()\n        alexnet_output = torch.reshape(alexnet_out, [32, 7, 1024])\n        # print(\"After reshape: \", alexnet_output.shape)\n        # conv_embedding = self.conv2Dblock1(x)\n        # print(\"shape after b1:\", conv_embedding.shape)\n        # conv_embedding = self.conv2Dblock2(conv_embedding)\n        # print(\"shape after b2:\", conv_embedding.shape)\n        # conv_embedding = self.conv2Dblock3(conv_embedding)\n        # print(\"shape after b3:\", conv_embedding.shape)\n        # conv_embedding = torch.flatten(conv_embedding, start_dim=2) # do not flatten batch dimension and time\n        # print(\"Conv embedding: \", conv_embedding.shape)\n        # lstm_embedding, (h,c) = self.lstm(conv_embedding)\n        lstm_embedding, (h,c) = self.lstm(alexnet_output)\n        lstm_embedding = self.dropout_lstm(lstm_embedding)\n        # lstm_embedding (batch, time, hidden_size*2)\n        # print(\"LSTM Embedding: \", lstm_embedding.shape)\n        batch_size,T,_ = lstm_embedding.shape \n        attention_weights = [None]*T\n        for t in range(T):\n            embedding = lstm_embedding[:,t,:]\n            attention_weights[t] = self.attention_linear(embedding)\n        attention_weights_norm = nn.functional.softmax(torch.stack(attention_weights,-1),dim=-1)\n        attention = torch.bmm(attention_weights_norm, lstm_embedding) # (Bx1xT)*(B,T,hidden_size*2)=(B,1,2*hidden_size)\n        attention = torch.squeeze(attention, 1)\n        output_logits = self.out_linear(attention)\n        output_softmax = nn.functional.softmax(output_logits,dim=1)\n        # print(output_logits.shape, output_softmax.shape, attention_weights_norm.shape)\n        return output_logits, output_softmax, attention_weights_norm","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:55:27.206579Z","iopub.execute_input":"2022-04-29T16:55:27.206858Z","iopub.status.idle":"2022-04-29T16:55:27.224533Z","shell.execute_reply.started":"2022-04-29T16:55:27.206822Z","shell.execute_reply":"2022-04-29T16:55:27.223893Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def loss_fnc(predictions, targets):\n    return nn.CrossEntropyLoss()(input=predictions,target=targets)\ndef make_train_step(model, loss_fnc, optimizer):\n    def train_step(X,Y):\n        # set model to train mode\n        model.train()\n        # forward pass\n        output_logits, output_softmax, attention_weights_norm = model(X)\n        predictions = torch.argmax(output_softmax,dim=1)\n        accuracy = torch.sum(Y==predictions)/float(len(Y))\n        # compute loss\n        loss = loss_fnc(output_logits, Y)\n        # compute gradients\n        loss.backward()\n        # update parameters and zero gradients\n        optimizer.step()\n        optimizer.zero_grad()\n        return loss.item(), accuracy*100\n    return train_step","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:55:27.225736Z","iopub.execute_input":"2022-04-29T16:55:27.226119Z","iopub.status.idle":"2022-04-29T16:55:27.239260Z","shell.execute_reply.started":"2022-04-29T16:55:27.226082Z","shell.execute_reply":"2022-04-29T16:55:27.238421Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def make_validate_fnc(model,loss_fnc):\n    def validate(X,Y):\n      EPOCHS=50\n      DATASET_SIZE = X_val.shape[0]\n      BATCH_SIZE = 32\n      device = 'cuda' if torch.cuda.is_available() else 'cpu'\n      losses = []\n      preds = []\n      accs = []\n      iters = int(DATASET_SIZE / BATCH_SIZE)\n      for i in range(iters):\n          batch_start = i * BATCH_SIZE\n          batch_end = min(batch_start + BATCH_SIZE, DATASET_SIZE)\n          actual_batch_size = batch_end-batch_start\n          x = X[batch_start:batch_end,:,:,:,:]\n          y = Y[batch_start:batch_end]\n          X_tensor = torch.tensor(x,device=device).float()\n          Y_tensor = torch.tensor(y, dtype=torch.long,device=device)\n          # loss, acc = train_step(X_tensor,Y_tensor)\n          # epoch_acc += acc*actual_batch_size/DATASET_SIZE\n          # epoch_loss += loss*actual_batch_size/DATASET_SIZE\n          # print(f\"\\r Epoch {epoch}: iteration {i}/{iters}\",end='')\n    \n          with torch.no_grad():\n              model.eval()\n              print(\"Valid X shape: \", x.shape)\n              output_logits, output_softmax, attention_weights_norm = model(x)\n              predictions = torch.argmax(output_softmax,dim=1)\n              accuracy = torch.sum(y==predictions)/float(len(y))\n              loss = loss_fnc(output_logits,y)\n              print(f\"Batch num: {i}\\nAccuracy: {accuracy}\")\n              losses.append(loss.item())\n              accs.append(accuracy.to('cpu'))\n              preds.extend(predictions.to('cpu'))\n      acc = torch.mean(torch.Tensor(accs))\n      loss = torch.sum(torch.Tensor(losses))\n      preds = torch.Tensor(preds)\n      return loss, acc*100, preds\n    return validate","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:55:27.240555Z","iopub.execute_input":"2022-04-29T16:55:27.241156Z","iopub.status.idle":"2022-04-29T16:55:27.253349Z","shell.execute_reply.started":"2022-04-29T16:55:27.241119Z","shell.execute_reply":"2022-04-29T16:55:27.252686Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"X_train = np.stack(mel_train_chunked,axis=0)\nX_train = np.expand_dims(X_train,2)\nprint('Shape of X_train: ',X_train.shape)\nX_val = np.stack(mel_val_chunked,axis=0)\nX_val = np.expand_dims(X_val,2)\nprint('Shape of X_val: ',X_val.shape)\nX_test = np.stack(mel_test_chunked,axis=0)\nX_test = np.expand_dims(X_test,2)\nprint('Shape of X_test: ',X_test.shape)\n\ndel mel_train_chunked\ndel mel_train\ndel mel_val_chunked\ndel mel_val\ndel mel_test_chunked\ndel mel_test","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:55:27.257302Z","iopub.execute_input":"2022-04-29T16:55:27.257720Z","iopub.status.idle":"2022-04-29T16:55:27.611464Z","shell.execute_reply.started":"2022-04-29T16:55:27.257668Z","shell.execute_reply":"2022-04-29T16:55:27.610738Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nb,t,c,h,w = X_train.shape\nX_train = np.reshape(X_train, newshape=(b,-1))\nX_train = scaler.fit_transform(X_train)\nX_train = np.reshape(X_train, newshape=(b,t,c,h,w))\n\nb,t,c,h,w = X_test.shape\nX_test = np.reshape(X_test, newshape=(b,-1))\nX_test = scaler.transform(X_test)\nX_test = np.reshape(X_test, newshape=(b,t,c,h,w))\n\nb,t,c,h,w = X_val.shape\nX_val = np.reshape(X_val, newshape=(b,-1))\nX_val = scaler.transform(X_val)\nX_val = np.reshape(X_val, newshape=(b,t,c,h,w))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:55:27.612581Z","iopub.execute_input":"2022-04-29T16:55:27.613006Z","iopub.status.idle":"2022-04-29T16:55:29.713548Z","shell.execute_reply.started":"2022-04-29T16:55:27.612967Z","shell.execute_reply":"2022-04-29T16:55:29.712768Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\ntype(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:55:29.714892Z","iopub.execute_input":"2022-04-29T16:55:29.715292Z","iopub.status.idle":"2022-04-29T16:55:29.728317Z","shell.execute_reply.started":"2022-04-29T16:55:29.715254Z","shell.execute_reply":"2022-04-29T16:55:29.727721Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"EPOCHS= 125\nDATASET_SIZE = X_train.shape[0]\nBATCH_SIZE = 32\nprint('Selected device is {}'.format(device))\nmodel = HybridModel(num_emotions=len(EMOTIONS)).to(device)\nprint('Number of trainable params: ',sum(p.numel() for p in model.parameters()))\n#OPTIMIZER = torch.optim.Adam(model.parameters(),lr=1e-5, weight_decay=1e-4)\n\nOPTIMIZER = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n\ntrain_step = make_train_step(model, loss_fnc, optimizer=OPTIMIZER)\nvalidate = make_validate_fnc(model,loss_fnc)\nlosses=[]\nval_losses = []\naccs = []\nval_accs = []\nfor epoch in range(EPOCHS):\n    # schuffle data\n    ind = np.random.permutation(DATASET_SIZE)\n    X_train = X_train[ind,:,:,:,:]\n    Y_train = Y_train[ind]\n    epoch_acc = 0\n    epoch_loss = 0\n    iters = int(DATASET_SIZE / BATCH_SIZE)\n    for i in range(iters):\n        batch_start = i * BATCH_SIZE\n        batch_end = min(batch_start + BATCH_SIZE, DATASET_SIZE)\n        actual_batch_size = batch_end-batch_start\n#         print(batch_start)\n#         print(batch_end)\n        X = X_train[batch_start:batch_end,:,:,:,:]\n        Y = Y_train[batch_start:batch_end]\n#         print(Y)\n#         print(Y.shape)\n#         print(type(Y))\n        X_tensor = torch.tensor(X,device=device).float()\n        Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n        loss, acc = train_step(X_tensor, Y_tensor)\n        epoch_acc += acc*actual_batch_size/DATASET_SIZE\n        epoch_loss += loss*actual_batch_size/DATASET_SIZE\n        print(f\"\\r Epoch {epoch}: iteration {i}/{iters}\",end='')\n    X_val_tensor = torch.tensor(X_val,device=device).float()\n    Y_val_tensor = torch.tensor(Y_val,dtype=torch.long,device=device)\n    val_loss, val_acc, _ = validate(X_val_tensor,Y_val_tensor)\n    accs.append(epoch_acc)\n    losses.append(epoch_loss)\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n    print('')\n    print(f\"Epoch {epoch} --> loss:{epoch_loss:.4f}, acc:{epoch_acc:.2f}%, val_loss:{val_loss:.4f}, val_acc:{val_acc:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-04-29T16:55:29.729654Z","iopub.execute_input":"2022-04-29T16:55:29.730133Z","iopub.status.idle":"2022-04-29T17:00:21.400347Z","shell.execute_reply.started":"2022-04-29T16:55:29.730099Z","shell.execute_reply":"2022-04-29T17:00:21.399601Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Model Statistics\nplt.plot(val_losses, label = 'valid_loss')\nplt.plot(losses, label = 'train_loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T17:00:21.401819Z","iopub.execute_input":"2022-04-29T17:00:21.402073Z","iopub.status.idle":"2022-04-29T17:00:21.587172Z","shell.execute_reply.started":"2022-04-29T17:00:21.402039Z","shell.execute_reply":"2022-04-29T17:00:21.586500Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Model Statistics\nplt.plot(val_accs, label = 'valid_accuracy')\nplt.plot(list(map(lambda x: x.cpu(), accs)), label = 'train_accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T17:00:21.588443Z","iopub.execute_input":"2022-04-29T17:00:21.588871Z","iopub.status.idle":"2022-04-29T17:00:21.792255Z","shell.execute_reply.started":"2022-04-29T17:00:21.588831Z","shell.execute_reply":"2022-04-29T17:00:21.791564Z"},"trusted":true},"execution_count":33,"outputs":[]}]}