{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "from utils import get_dataset\n",
    "from SpeechModel import SpeechModel\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from os import mkdir, path\n",
    "import argparse\n",
    "# parser = argparse.ArgumentParser(description=\"Script to train the model as described in the paper.\")\n",
    "# parser.add_argument(\"epochs\", type=int,  help=\"Number of Epochs\")\n",
    "\n",
    "# parser.add_argument(\"-nc\", type=bool, help=\"Disable caching. Enabled by default.\")\n",
    "\n",
    "# # args for batchsize, data_directory, validation_split, random state, etc\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# if args.epochs:\n",
    "#     EPOCHS = args.epochs\n",
    "# else:\n",
    "#     EPOCHS = 10\n",
    "\n",
    "# if args.nc:\n",
    "#     CACHE= False\n",
    "# else:\n",
    "#     CACHE = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_framed_mel_spectrograms(wav, sr=22050):\n",
    "    # The duration of clips is 3 seconds, ie. 3000 miliseconds. Do some quick math to figure out frame_length.\n",
    "    frame_length = tf.cast(sr * (25 / 1000), tf.int32)  # 25 ms\n",
    "    frame_step = tf.cast(sr * (10 / 1000), tf.int32)  # 10 ms\n",
    "    stft_out = tf.signal.stft(\n",
    "        wav,\n",
    "        frame_length=frame_length,\n",
    "        frame_step=frame_step,\n",
    "        window_fn=tf.signal.hamming_window,\n",
    "    )\n",
    "    num_spectrogram_bins = tf.shape(stft_out)[-1]\n",
    "    stft_abs = tf.abs(stft_out)\n",
    "    lower_edge_hz, upper_edge_hz = 20.0, 8000.0\n",
    "    num_mel_bins = 64\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "        num_mel_bins, num_spectrogram_bins, sr, lower_edge_hz, upper_edge_hz\n",
    "    )\n",
    "    mel_spectrograms = tf.tensordot(stft_abs, linear_to_mel_weight_matrix, 1)\n",
    "\n",
    "    # mel_spectrograms.set_shape(\n",
    "    #     stft_abs.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:])\n",
    "    # )\n",
    "\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "    log_mel_d1 = log_mel_spectrograms - \\\n",
    "        tf.roll(log_mel_spectrograms, -1, axis=0)\n",
    "    log_mel_d2 = log_mel_d1 - tf.roll(log_mel_d1, -1, axis=0)\n",
    "\n",
    "    log_mel_three_channel = tf.stack(\n",
    "        [log_mel_spectrograms, log_mel_d1, log_mel_d2], axis=-1\n",
    "    )\n",
    "\n",
    "    framed_log_mels = tf.signal.frame(\n",
    "        log_mel_three_channel, frame_length=64, frame_step=32, pad_end=False, axis=0\n",
    "    )\n",
    "\n",
    "    return framed_log_mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 22:31:34.711138: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa\n",
    "import numpy as np\n",
    "CACHE = True\n",
    "EMOTION_DICT_RAVDEES = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\",\n",
    "}\n",
    "def load_wav(file_path):\n",
    "    file_path = file_path.numpy()\n",
    "    wav, sr = librosa.load(file_path, mono=True, duration=3)\n",
    "\n",
    "    pre_emp = 0.97\n",
    "    wav = np.append(wav[0], wav[1:] - pre_emp * wav[:-1])\n",
    "\n",
    "    wav = tf.convert_to_tensor(wav, dtype=tf.float32)\n",
    "    sr = tf.convert_to_tensor(sr, dtype=tf.float32)\n",
    "    return wav, sr\n",
    "def get_dataset(DATA_DIR: str, cache: bool = True):\n",
    "    def decompose_label(file_path: str):\n",
    "        return label_to_int[file_path.split(\"-\")[2]]\n",
    "\n",
    "    def tf_compatible_file_loader(file_path):\n",
    "        wav, sr = tf.py_function(load_wav, [file_path], [\n",
    "                                 tf.float32, tf.float32])\n",
    "        return wav, sr\n",
    "\n",
    "    file_path_list = os.listdir(DATA_DIR)\n",
    "    label_to_int = dict({(key, i)\n",
    "                        for i, key in enumerate(EMOTION_DICT_RAVDEES.keys())})\n",
    "    # print(len(file_path_list))\n",
    "    # print(file_path_list[768].split(\"-\"))\n",
    "    labels = [decompose_label(file_path) for file_path in file_path_list]\n",
    "    file_path_list = [DATA_DIR + \"/\" +\n",
    "                      file_path for file_path in file_path_list]\n",
    "    \n",
    "    train_fps, val_fps, train_labels, val_labels = train_test_split(\n",
    "        file_path_list, labels, test_size=0.1\n",
    "    )\n",
    "    # print(\"train_fps:\", len(train_fps))\n",
    "    # print(\"val_fps:\", len(val_fps))\n",
    "    # print(\"train labels\", len(train_labels))\n",
    "    # print(\"validation labels\", len(val_labels))\n",
    "    train_files_ds = tf.data.Dataset.from_tensor_slices(train_fps)\n",
    "    train_wav_ds = train_files_ds.map(\n",
    "        tf_compatible_file_loader,  num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    train_mfcc_ds = train_wav_ds.map(\n",
    "        get_framed_mel_spectrograms,  num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    train_labels_ds = tf.data.Dataset.from_tensor_slices(train_labels)\n",
    "\n",
    "    train_ds = tf.data.Dataset.zip((train_mfcc_ds, train_labels_ds))\n",
    "\n",
    "    val_files_ds = tf.data.Dataset.from_tensor_slices(val_fps)\n",
    "    val_wav_ds = val_files_ds.map(\n",
    "        tf_compatible_file_loader,  num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    val_mfcc_ds = val_wav_ds.map(\n",
    "        get_framed_mel_spectrograms,  num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    val_labels_ds = tf.data.Dataset.from_tensor_slices(val_labels)\n",
    "\n",
    "    val_ds = tf.data.Dataset.zip((val_mfcc_ds, val_labels_ds))\n",
    "\n",
    "    if cache:\n",
    "        train_ds = train_ds.batch(32).prefetch(tf.data.AUTOTUNE).cache()\n",
    "        val_ds = val_ds.batch(32).prefetch(tf.data.AUTOTUNE).cache()\n",
    "    else:\n",
    "        train_ds = train_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "        val_ds = val_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds\n",
    "train_ds , validation_ds = get_dataset(\"dataset\", cache=CACHE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ResNet Weights\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "\n",
    "MODEL_SAVE_DIR = \"saved_model\"\n",
    "\n",
    "\n",
    "\n",
    "SP = SpeechModel()\n",
    "model = SP.create_model()\n",
    "\n",
    "ESCallback = EarlyStopping(patience=5, restore_best_weights=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 22:32:11.927615: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"109\" frequency: 1600 num_cores: 4 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 3145728 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - ETA: 0s - loss: 2.0662 - acc: 0.1497 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 22:58:29.067075: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"109\" frequency: 1600 num_cores: 4 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 3145728 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 1620s 39s/step - loss: 2.0662 - acc: 0.1497 - val_loss: 2.0659 - val_acc: 0.1389\n",
      "Epoch 2/30\n",
      "41/41 [==============================] - 1839s 45s/step - loss: 1.9731 - acc: 0.2739 - val_loss: 1.9083 - val_acc: 0.3264\n",
      "Epoch 3/30\n",
      "41/41 [==============================] - 1601s 39s/step - loss: 1.6592 - acc: 0.4028 - val_loss: 1.5289 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "41/41 [==============================] - 1584s 39s/step - loss: 1.2223 - acc: 0.5586 - val_loss: 1.4842 - val_acc: 0.5208\n",
      "Epoch 5/30\n",
      "41/41 [==============================] - 1546s 38s/step - loss: 0.7326 - acc: 0.7724 - val_loss: 1.2958 - val_acc: 0.6042\n",
      "Epoch 6/30\n",
      "41/41 [==============================] - 1554s 38s/step - loss: 0.3793 - acc: 0.8819 - val_loss: 1.3440 - val_acc: 0.5764\n",
      "Epoch 7/30\n",
      "41/41 [==============================] - 1579s 39s/step - loss: 0.2160 - acc: 0.9390 - val_loss: 1.4062 - val_acc: 0.5694\n",
      "Epoch 8/30\n",
      "41/41 [==============================] - 1430s 35s/step - loss: 0.1038 - acc: 0.9799 - val_loss: 1.6247 - val_acc: 0.5833\n",
      "Epoch 9/30\n",
      "41/41 [==============================] - 1484s 36s/step - loss: 0.0739 - acc: 0.9830 - val_loss: 1.8461 - val_acc: 0.5556\n",
      "Epoch 10/30\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.0701 - acc: 0.9823 Restoring model weights from the end of the best epoch: 5.\n",
      "41/41 [==============================] - 1674s 41s/step - loss: 0.0701 - acc: 0.9823 - val_loss: 1.7635 - val_acc: 0.6111\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 02:58:17.447665: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "2022-04-26 03:01:17.025667: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at save_restore_v2_ops.cc:136 : RESOURCE_EXHAUSTED: saved_model/30epochs_SpeechModel/variables/variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate16237429839950581410; No space left on device\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "saved_model/30epochs_SpeechModel/variables/variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate16237429839950581410; No space left on device [Op:SaveV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/30/rh07w67s0_75f3slk_2msn4h0000gn/T/ipykernel_28413/3327221031.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_SAVE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_SAVE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_SAVE_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"epochs_SpeechModel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: saved_model/30epochs_SpeechModel/variables/variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate16237429839950581410; No space left on device [Op:SaveV2]"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=EPOCHS, validation_data=validation_ds, callbacks=[ESCallback])\n",
    "\n",
    "if not path.exists(MODEL_SAVE_DIR):\n",
    "    mkdir(MODEL_SAVE_DIR)\n",
    "model.save(MODEL_SAVE_DIR + \"/\" + str(EPOCHS) + \"epochs_SpeechModel\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
